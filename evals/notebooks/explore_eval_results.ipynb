{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Evaluation Results\n",
    "\n",
    "This notebook provides interactive exploration of evaluation data collected from the resume optimizer pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd().parent.parent\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from evals.db.eval_db import EvalDatabase\n",
    "from evals.framework.analyzer import EvalAnalyzer\n",
    "from evals.framework.config_resume import get_resume_eval_config, RESUME_STAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize database and analyzer\n",
    "config = get_resume_eval_config()\n",
    "db = EvalDatabase(config.db_path)\n",
    "analyzer = EvalAnalyzer(db)\n",
    "\n",
    "print(f\"Database: {config.db_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count scenarios and evaluations\n",
    "scenarios = db.list_scenarios(limit=1000)\n",
    "print(f\"Total scenarios: {len(scenarios)}\")\n",
    "\n",
    "for stage_id in RESUME_STAGES:\n",
    "    judgments = db.get_judgments_for_stage(stage_id)\n",
    "    print(f\"  {stage_id}: {len(judgments)} judgments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Win Rates by Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select stage to analyze\n",
    "stage_id = \"optimizer\"  # Change as needed\n",
    "\n",
    "win_rates = analyzer.compute_win_rates(stage_id)\n",
    "\n",
    "if win_rates:\n",
    "    df = pd.DataFrame([\n",
    "        {\n",
    "            \"Model\": r.model_id.split(\"/\")[-1],\n",
    "            \"Win Rate\": r.win_rate,\n",
    "            \"Wins\": r.wins,\n",
    "            \"Appearances\": r.appearances,\n",
    "        }\n",
    "        for r in win_rates\n",
    "    ])\n",
    "    display(df)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.barplot(data=df, x=\"Model\", y=\"Win Rate\", ax=ax)\n",
    "    ax.set_title(f\"Win Rates for {stage_id} Stage\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No evaluation data available for this stage.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bradley-Terry Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bt_results = analyzer.bradley_terry_ranking(stage_id)\n",
    "\n",
    "if bt_results:\n",
    "    bt_df = pd.DataFrame([\n",
    "        {\n",
    "            \"Rank\": r.rank,\n",
    "            \"Model\": r.model_id.split(\"/\")[-1],\n",
    "            \"Strength\": r.strength,\n",
    "        }\n",
    "        for r in bt_results\n",
    "    ])\n",
    "    display(bt_df)\n",
    "else:\n",
    "    print(\"Insufficient data for Bradley-Terry ranking.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pairwise Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise = analyzer.all_pairwise_comparisons(stage_id)\n",
    "\n",
    "if pairwise:\n",
    "    pw_df = pd.DataFrame([\n",
    "        {\n",
    "            \"Model A\": r.model_a.split(\"/\")[-1],\n",
    "            \"Model B\": r.model_b.split(\"/\")[-1],\n",
    "            \"P(A > B)\": r.p_a_preferred,\n",
    "            \"CI Low\": r.ci_low,\n",
    "            \"CI High\": r.ci_high,\n",
    "            \"Significant\": r.significant,\n",
    "            \"N\": r.total,\n",
    "        }\n",
    "        for r in pairwise\n",
    "    ])\n",
    "    display(pw_df)\n",
    "else:\n",
    "    print(\"No pairwise data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Score Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores = analyzer.compute_mean_scores(stage_id)\n",
    "\n",
    "if mean_scores:\n",
    "    scores_data = []\n",
    "    for model, criteria in mean_scores.items():\n",
    "        for criterion, score in criteria.items():\n",
    "            scores_data.append({\n",
    "                \"Model\": model.split(\"/\")[-1],\n",
    "                \"Criterion\": criterion,\n",
    "                \"Score\": score,\n",
    "            })\n",
    "    \n",
    "    scores_df = pd.DataFrame(scores_data)\n",
    "    \n",
    "    # Pivot for heatmap\n",
    "    pivot = scores_df.pivot(index=\"Model\", columns=\"Criterion\", values=\"Score\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    sns.heatmap(pivot, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", ax=ax, vmin=1, vmax=5)\n",
    "    ax.set_title(f\"Mean Scores by Criterion ({stage_id})\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No score data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Tag Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_freqs = analyzer.compute_tag_frequencies(stage_id)\n",
    "\n",
    "if tag_freqs:\n",
    "    tag_data = []\n",
    "    for model, tags in tag_freqs.items():\n",
    "        for tag, count in tags.items():\n",
    "            tag_data.append({\n",
    "                \"Model\": model.split(\"/\")[-1],\n",
    "                \"Tag\": tag,\n",
    "                \"Count\": count,\n",
    "            })\n",
    "    \n",
    "    tag_df = pd.DataFrame(tag_data)\n",
    "    display(tag_df)\n",
    "else:\n",
    "    print(\"No tag data available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "report = analyzer.generate_report(stage_id)\n",
    "\n",
    "# Save to file\n",
    "output_path = f\"../results/{stage_id}_report.json\"\n",
    "Path(output_path).parent.mkdir(exist_ok=True)\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(f\"Report saved to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
